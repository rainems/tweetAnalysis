{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv('../sentiment_training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') + ['and']\n",
    "\n",
    "def sanitize(tweet_text):\n",
    "    # remove urls from tweet string\n",
    "    tweet_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet_text)\n",
    "    # strip punctuation\n",
    "    tweet_text = re.sub(r'[^a-zA-Z]', ' ', tweet_text)\n",
    "    # strip emojiis\n",
    "    emoji_pattern = re.compile(\n",
    "        u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "        u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "        u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "        u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "        u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "        \"+\", flags=re.UNICODE)\n",
    "    tweet_text = emoji_pattern.sub(r'', tweet_text)\n",
    "    # remove stop words from nltk corpus\n",
    "    tweet_text = ' '.join(w for w in tweet_text.strip().lower().split() if not w in stopwords)\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/inspect.py\", line 1048, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/inspect.py\", line 1008, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/inspect.py\", line 453, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/inspect.py\", line 490, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/py/_apipkg.py\", line 171, in __getattribute__\n",
      "    return getattr(getmod(), name)\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/py/_apipkg.py\", line 155, in getmod\n",
      "    x = importobj(modpath, None)\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/py/_apipkg.py\", line 48, in importobj\n",
      "    module = __import__(modpath, None, None, ['__doc__'])\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/pytest.py\", line 21, in <module>\n",
      "    from _pytest.config import (\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/_pytest/config.py\", line 12, in <module>\n",
      "    import _pytest.hookspec  # the extension point definitions\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/_pytest/hookspec.py\", line 3, in <module>\n",
      "    from _pytest._pluggy import HookspecMarker\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/_pytest/_pluggy.py\", line 7, in <module>\n",
      "    from _pytest.vendored_packages.pluggy import *  # noqa\n",
      "  File \"/home/eldon/anaconda2/lib/python2.7/site-packages/_pytest/vendored_packages/pluggy.py\", line 66, in <module>\n",
      "    \"\"\"\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sanitize the input data\n",
    "df['text'] = df['text'].map(sanitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, max_df=0.8, sublinear_tf=True, use_idf=True, decode_error='ignore')\n",
    "train_vectors = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "cross_val_score(clf, train_vectors, df['sentiment']) # 3 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "cross_val_score(clf_rf, train_vectors, df['sentiment']) # 3 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_lin = SGDClassifier()\n",
    "cross_val_score(clf_sgd, train_vectors, df['sentiment']) # 3 fold cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
